{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"BARI Environmental Sensor Network Documentation","text":""},{"location":"#welcome","title":"Welcome","text":"<p>This documentation describes the technical infrastructure supporting the Common SENSES Project sensor data pipeline. Common SENSES is a community-led environmental justice initiative serving neighborhoods along Blue Hill Avenue from Dudley Town Common to Franklin Park in Boston.</p>"},{"location":"#about-common-senses","title":"About Common SENSES","text":"<p>Common SENSES is a collaboration between Dudley Street Neighborhood Initiative, Project R.I.G.H.T. Inc., Boston Area Research Initiative (BARI) at Northeastern University, and the City of Boston's Office of Emerging Technology.</p> <p>Project Values:</p> <ul> <li>No Surveillance - Only environmental data, no information about people</li> <li>Environmental Justice - Supporting community action on environmental issues</li> <li>Data Justice - All data publicly accessible for community use</li> </ul> <p>Learn more at commonsensesproject.org</p>"},{"location":"#project-status","title":"Project Status","text":"<p>Current State: Fully operational production system Deployment Date: December 2025 Last Updated: January 2026 Active Sensors: 55 sensors collecting data every 15 minutes Data Volume: ~79,000 readings/day, ~2.4M readings/month Monthly Cost: &lt;$80 (Azure infrastructure)</p>"},{"location":"#what-this-system-does","title":"What This System Does","text":"<p>The sensor data pipeline collects, processes, stores, and visualizes environmental data from Particle.io sensors deployed throughout the Common SENSES project area. The system measures temperature, humidity, and noise levels every 15 minutes, supporting both community access and research analysis.</p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"<p>New to the project? Start with the Project Overview for a comprehensive one-page summary.</p> <p>Need to access or analyze data? Go to Working with Data for database queries and data export methods.</p> <p>Maintaining the system? See Operating the System for daily tasks and troubleshooting.</p> <p>Taking over development? Read Understanding the System, then explore Reference for complete technical details.</p> <p>Need step-by-step instructions? Check the Appendices for Azure configuration, database management, and deployment guides.</p>"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#overview","title":"Overview","text":"<p>One-page system summary with architecture, data flow, and key components.</p>"},{"location":"#understanding-the-system","title":"Understanding the System","text":"<ul> <li>What It Does - High-level purpose</li> <li>Architecture - Component interactions</li> <li>Data Flow - Sensor to dashboard journey</li> <li>Design Decisions - Why this architecture?</li> </ul>"},{"location":"#working-with-data","title":"Working with Data","text":"<ul> <li>Quick Start - Get data immediately</li> <li>Connecting to Database - Connection strings and access</li> <li>Common Queries - Copy-paste SQL examples</li> <li>Blob Storage Access - Raw JSON files</li> <li>Understanding the Schema - Tables and relationships</li> <li>Data Quality - Flags and validation</li> </ul>"},{"location":"#operating-the-system","title":"Operating the System","text":"<ul> <li>Daily Checklist - Routine maintenance tasks</li> <li>Monitoring Health - System health indicators</li> <li>Cost Tracking - Budget monitoring</li> <li>Sensor Lifecycle - Deploy, move, deactivate</li> <li>Troubleshooting:</li> <li>Sensor Not Reporting</li> <li>Function Errors</li> <li>Database Issues</li> <li>Dashboard Problems</li> </ul>"},{"location":"#making-changes","title":"Making Changes","text":"<ul> <li>Deploying Code Updates - Function deployment</li> <li>Updating Dashboard - Static Web App changes</li> <li>Database Changes - Schema migrations</li> <li>Sensor Configuration - Particle.io webhooks</li> <li>Infrastructure Changes - Azure resource modifications</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Azure Infrastructure:</li> <li>Function Apps</li> <li>SQL Database</li> <li>Blob Storage</li> <li>Networking</li> <li>Static Web App</li> <li>Particle Platform - Device specs and webhook config</li> <li>Complete Schema - All tables, columns, indexes</li> <li>Function Code Reference - Code organization</li> <li>API Endpoints - Dashboard API documentation</li> </ul>"},{"location":"#appendices","title":"Appendices","text":"<ul> <li>Step-by-Step Guides - Azure Portal tasks with screenshots</li> <li>SQL Query Library - Extended query examples</li> <li>Error Codes - Error reference</li> <li>Contacts - Who to contact for support</li> </ul>"},{"location":"#external-resources","title":"External Resources","text":"<ul> <li>Common SENSES Project Website</li> <li>Common SENSES Research Map</li> <li>Heat Live Public Dashboard</li> <li>Noise Live Public Dashboard</li> </ul>"},{"location":"00-overview/","title":"BARI Environmental Sensor Network - Project Overview","text":""},{"location":"00-overview/#what-this-system-does","title":"What This System Does","text":"<p>The BARI Environmental Sensor Network monitors environmental conditions in Boston's Blue Hill Avenue corridor through 55 Particle.io sensors deployed in partnership with community organizations as part of the Common SENSES Project. Every 15 minutes, these sensors collect temperature, humidity, and noise data, which flows through an automated Azure-based pipeline to provide both research-grade historical data and real-time public accessibility through an interactive dashboard.</p> <p>Quick Facts: - 55 active sensors in the Common SENSES project area (Blue Hill Avenue corridor) - ~290,000 readings collected per day - 15-minute collection intervals - Data stored in UTC, displayed in US/Eastern timezone - &lt;$70/month operational cost - Public dashboard + research database</p>"},{"location":"00-overview/#system-architecture","title":"System Architecture","text":""},{"location":"00-overview/#data-flow","title":"Data Flow","text":"<p>1. Data Collection \u2192 2. Ingestion \u2192 3. Storage \u2192 4. Processing \u2192 5. Database \u2192 6. Visualization</p> <ol> <li>Sensors (Particle.io devices) measure temperature, humidity, and noise every 15 minutes</li> <li>Particle Cloud receives sensor readings and triggers webhooks</li> <li>Webhook Function (Azure) receives JSON payloads and writes raw data to Blob Storage</li> <li>Blob Storage archives data and triggers the processor function</li> <li>Blob Processor Function deduplicates, validates, and writes cleaned data to SQL Database</li> <li>SQL Database stores validated readings with full audit trail</li> <li>API Function serves data requests from the dashboard</li> <li>Public Dashboard (Static Web App) displays interactive time-series visualizations</li> </ol> <p>Data Transfer Types: - Solid arrows (\u2192) = Push/Write operations (HTTPS POST, writes) - Dashed arrows (\u21e2) = Pull/Trigger operations (blob triggers, SQL queries, API calls)</p>"},{"location":"00-overview/#key-components","title":"Key Components","text":""},{"location":"00-overview/#azure-infrastructure","title":"Azure Infrastructure","text":"<p>Function Apps (4 functions): - Webhook Receiver: Accepts Particle Cloud webhooks, archives raw JSON to blob storage - Blob Processor: Reads from blob storage, performs multi-layer deduplication, writes to SQL - API Function: Serves sensor data requests to the public dashboard - Daily Reporter: Automated email summaries of sensor performance (monitoring/operations)</p> <p>SQL Database: - Tables: <code>sensor_readings</code>, <code>sensor_errors</code>, <code>sensor_startup_events</code>, <code>sensor_installs</code> - Composite indexes on <code>(device_id, published_at)</code> for query performance - Computed columns for data quality metrics - Private endpoint access only (secure)</p> <p>Blob Storage: - Organized folders: <code>incoming/</code>, <code>archived/</code>, <code>failed-writing/</code>, <code>duplicated/</code>, <code>overlapping/</code> - Raw JSON preservation for audit trail - Blob-triggered processing pipeline</p> <p>Static Web App: - Vanilla HTML/CSS/JavaScript (no frameworks for easier handoff) - Plotly.js for interactive time-series charts - Flatpickr for date range selection - Caching headers to minimize API calls and data egress costs</p> <p>Networking: - VNet with private endpoints for SQL and Blob Storage - Public ingress: webhook endpoint, Static Web App - All internal traffic secured through private network</p>"},{"location":"00-overview/#particleio-platform","title":"Particle.io Platform","text":"<ul> <li>55 devices configured with webhook integration</li> <li>Real-time data transmission to Azure</li> <li>Device health monitoring and alerts</li> <li>Webhook retry logic for reliability</li> </ul>"},{"location":"00-overview/#data-quality-reliability","title":"Data Quality &amp; Reliability","text":"<p>The system implements multiple layers of protection:</p> <p>Deduplication (3 levels): 1. Webhook level: Reject exact duplicates within 5-second window 2. Batch level: Remove duplicates within blob file 3. Database level: Composite unique constraint on <code>(device_id, published_at)</code></p> <p>Error Handling: - Failed webhook writes \u2192 <code>failed-writing/</code> folder for manual review - Duplicate detections \u2192 <code>duplicated/</code> folder with metadata - Overlapping time ranges \u2192 <code>overlapping/</code> folder - All errors logged to <code>sensor_errors</code> table</p> <p>Monitoring: - Daily automated reports via email - Missing data analysis by sensor - Application Insights for function performance - Cost tracking and alerts (warning at $90, critical at $100)</p>"},{"location":"00-overview/#cost-management","title":"Cost Management","text":"<p>Monthly operational costs kept under $80 through: - Efficient SQL query patterns with proper indexing - API response caching (24-hour headers) - Reduced Application Insights logging verbosity - Serverless Function App consumption plan (Flex Consumption tier) - Basic tier SQL Database with appropriate sizing</p> <p>Cost Monitoring: - Cost alerts configured at $90 (warning) and $100 (critical) - Monthly reviews via Azure Cost Management dashboard - See Cost Tracking for detailed tracking procedures</p>"},{"location":"00-overview/#public-dashboard","title":"Public Dashboard","text":"<p>Available at: {{ no such element: super_collections.SuperDict object['dashboard_public'] }}</p> <p>Features: - Interactive time-series charts for temperature, humidity, noise - Flexible date range selection - Per-sensor data visualization - Real-time data updates (15 minutes granularity) - Mobile-responsive design - No authentication required (public data for community transparency)</p>"},{"location":"00-overview/#key-design-principles","title":"Key Design Principles","text":"<p>Maintainability First: - Vanilla JavaScript over frameworks (easier handoff) - Clear code organization and commenting - Comprehensive documentation - Modular architecture</p> <p>Data Integrity: - Multiple deduplication layers - Complete audit trail via blob storage - Defensive programming with input validation - Graceful error handling</p> <p>Cost Efficiency: - Strategic caching to minimize API calls and data egress - Efficient database indexing - Serverless architecture - Careful monitoring of Azure costs</p> <p>Defensive Programming: - Input validation at every stage - Connection cleanup and timeout handling - Comprehensive error logging - Retry logic where appropriate</p>"},{"location":"00-overview/#who-should-use-this-documentation","title":"Who Should Use This Documentation","text":"<p>System Administrators: Start with Daily Checklist for daily tasks and Troubleshooting</p> <p>Developers/Future Maintainers: Read Architecture and Function Code Reference to understand implementation</p> <p>Researchers: See Quick Start for getting data immediately and Common Queries for typical analysis tasks</p> <p>IT Support: Check Step-by-Step Guides for Azure Portal management tasks</p>"},{"location":"00-overview/#navigation-guide","title":"Navigation Guide","text":"<p>Understanding the System: - What It Does - Project background, Common SENSES context, stakeholders - Architecture - Detailed architecture with network topology - Data Flow - How data moves through the pipeline - Design Decisions - Why we built it this way</p> <p>Working with Data: - Quick Start - Get data right now - Connecting to Database - Connection strings and authentication - Common Queries - Copy-paste SQL for typical tasks - Understanding the Schema - Table relationships and columns</p> <p>Operating the System: - Daily Checklist - What to check every day - Monitoring Health - System health indicators - Cost Tracking - Where costs come from - Troubleshooting - Common issues and fixes</p> <p>Making Changes: - Deploying Code Updates - Push function changes - Database Changes - Schema migrations - Sensor Configuration - Particle.io webhook settings</p> <p>Technical Reference: - Azure Infrastructure - Complete Azure resource details - Complete Schema - Every table, column, index - Function Code Reference - Code organization - Particle Platform - Device specs</p> <p>Step-by-Step Guides: - Appendices - Screenshots and detailed procedures</p>"},{"location":"00-overview/#project-status","title":"Project Status","text":"<p>Current State: Fully operational production system Deployment Date: December 2025 Last Major Update: January 2026 Active Sensors: 55 sensors currently collecting data in the Common SENSES project area Data Volume: ~79,000 readings/day, ~2.4M readings/month Budget Status: Within target (&lt;$80/month)</p>"},{"location":"00-overview/#key-contacts","title":"Key Contacts","text":"<p>Research Team:</p> <ul> <li> <p>Dan (Technical guidance and research direction)</p> </li> <li> <p>Amy (Sensor deployment coordination)</p> </li> </ul> <p>IT Support:</p> <p>Project Lead &amp; Documentation: - Juan Cruz (System design, implementation, handoff)</p> <p>Community Partners:</p> <ul> <li> <p>Dudley Street Neighborhood Initiative</p> </li> <li> <p>Project R.I.G.H.T. Inc.</p> </li> </ul> <p>For email addresses, Slack handles, and phone numbers, see Contact Information</p> <p>Next Steps: - New to the project? Read What It Does for Common SENSES context - Need to perform a task? Jump to Daily Checklist - Want technical details? Start with Architecture - Need to access data? See Quick Start</p>"},{"location":"01-understanding-the-system/architecture/","title":"System Architecture","text":"<p>This page provides a high-level view of the BARI Environmental Sensor Network infrastructure. It shows the major components, how they connect, and how data flows from sensors in the field to the public dashboard. For detailed technical specifications of individual components, see the Reference section. For step-by-step data processing, see Data Flow.</p>"},{"location":"01-understanding-the-system/architecture/#architecture-overview","title":"Architecture Overview","text":""},{"location":"01-understanding-the-system/architecture/#core-components","title":"Core Components","text":"<p> Particle Sensors</p> <p>55 environmental sensors deployed throughout the Common SENSES project area. Each sensor collects temperature, humidity, and noise data every 15 minutes and transmits readings to Particle Cloud via cellular connection.</p> <p>For technical specifications see Particle Platform Reference.</p> <p> Particle Cloud</p> <p>Particle.io's cloud platform receives data from sensors and triggers webhooks to deliver readings to Azure infrastructure.</p> <p>For webhook configuration details see Particle Platform Reference.</p> <p> Webhook Receiver</p> <p>Azure Function that receives JSON payloads from Particle webhooks, validates and parses sensor data, then writes to Blob Storage as individual files. Each webhook delivery contains multiple sensor readings grouped together.</p> <p>For function code and configuration see Function Apps Reference.</p> <p> Blob Storage</p> <p>Azure Blob Storage container organized into folders (incoming/, archived/, failed-writing/) that stores raw sensor data as JSON files for downstream processing.</p> <p>For storage organization and access see Blob Storage Reference.</p> <p> Database Writer</p> <p>Azure Function that runs every 30 minutes to process files in the incoming/ folder. Validates, deduplicates, and writes sensor readings to the SQL database, then moves processed files to archived/ or failed-{reason}/ folders.</p> <p>For function code and configuration see Function Apps Reference.</p> <p> SQL Database</p> <p>Azure SQL Database that stores validated sensor readings, error logs, startup events, and sensor deployment records. Primary data source for research analysis and dashboard queries.</p> <p>For complete schema details see Complete Schema Reference and SQL Database Reference.</p> <p> API</p> <p>Azure Function that provides HTTP endpoints for querying sensor data. Serves the public dashboard with filtered, aggregated data based on time ranges and sensor selections.</p> <p>For endpoint documentation see API Endpoints Reference and Function Apps Reference.</p> <p> Public Facing Dashboard</p> <p>Azure Static Web App hosting the public dashboard at link for heat data and link for noise data. Built with vanilla JavaScript and Plotly.js for data visualization.</p> <p>For dashboard code and deployment see Static Web App Reference.</p> <p> Daily Reporter</p> <p>Azure Function that runs on a daily schedule to send operational summaries via email. Reports include sensor health status, data collection metrics, and any errors from the previous 24 hours.</p> <p>For function code and configuration see Function Apps Reference.</p>"},{"location":"01-understanding-the-system/architecture/#component-interactions","title":"Component Interactions","text":"<p> Particle Sensors \u2192  Particle Cloud</p> <p>Sensors transmit readings via cellular connection every 15 minutes.</p> <p> Particle Cloud \u2192  Webhook Receiver</p> <p>Particle Cloud pushes sensor data to Webhook Receiver via HTTP webhook.</p> <p> Webhook Receiver \u2192  Blob Storage</p> <p>Webhook Receiver writes validated JSON files to incoming/ folder in Blob Storage.</p> <p> Blob Storage \u21e2  Database Writer</p> <p>Database Writer polls incoming/ folder on schedule to retrieve unprocessed files.</p> <p> Database Writer \u2192  SQL Database</p> <p>Database Writer validates, deduplicates, and writes sensor readings to database tables.</p> <p> Public Facing Dashboard \u2192  API</p> <p>Dashboard sends requests to API endpoints based on user-selected time ranges and sensors.</p> <p> SQL Database \u21e2  API</p> <p>API Function queries database and returns filtered, aggregated sensor readings.</p> <p> SQL Database \u21e2  Daily Reporter</p> <p>Daily Reporter queries operational metrics and sends summary email reports on schedule.</p>"},{"location":"01-understanding-the-system/data-flow/","title":"Data Flow","text":"<p>Brief paragraph explaining the contents of this section</p>"},{"location":"01-understanding-the-system/data-flow/#architecture-overview","title":"Architecture Overview","text":""},{"location":"01-understanding-the-system/data-flow/#data-flow_1","title":"Data Flow","text":"<p> Particle Sensors \u2192  Particle Cloud</p> <p>Sensors collect temperature, relative humidity, and noise readings every minute. Data is batched and transmitted every 15 minutes via cellular connection.</p> <p>Data format: Comma-separated string containing timestamp, Box ID, and 15 sets of raw readings (T, RH, Noise).</p> <p>Example message: <pre><code>,19,6,6,17,34,2483,5480,6389,...\n</code></pre></p> <p>Data transformations needed: Temperature and humidity divide by 100, noise applies formula. See Particle Platform Reference for complete specifications.</p> <p> Particle Cloud \u2192  Webhook Receiver</p> <p>Particle Cloud wraps the sensor message in a JSON payload and sends it to the Webhook Receiver via HTTP POST request. The Webhook Receiver exposes an HTTP endpoint. If the function is unavailable or unreachable, Particle Cloud retries briefly before data is lost.</p> <p>Data format: JSON object containing: <pre><code>{\n    \"event\": \"&lt;event-name&gt;\",\n    \"data\": \",19,6,6,17,34,2483,5480,6389,...\",\n    \"published_at\": \"2025-06-24T14:39:44.391Z\",\n    \"coreid\": \"&lt;core-id&gt;\",\n    \"userid\": \"&lt;user-id&gt;\",\n    \"fw_version\": \"&lt;fw_version&gt;\",\n    \"public\": false\n}\n</code></pre></p> <p>The <code>data</code> field contains the raw sensor message. Other fields provide metadata used for validation and duplicate detection.</p> <p>For webhook configuration see Particle Platform Reference. For endpoint configuration see Function Apps Reference.</p> <p> Webhook Receiver \u2192  Blob Storage</p> <p>The Webhook Receiver is the entry point to the Azure infrastructure. After validating incoming data, it detects whether the message contains environmental readings, startup logs, or error messages, then parses accordingly. For environmental data, it extracts the box_id, initial timestamp for the batch, and raw triplets of data per minute (T, RH, Noise). The parsed data along with the original raw string are written to the <code>environment/incoming/</code> folder in Blob Storage for database processing.</p> <p>Data format: JSON file with parsed environmental data: <pre><code>{\n    \"datatype\": \"environment\",\n    \"raw\": \",19,6,6,17,34,2483,5480,6389,...\",\n    \"parsed_at\": \"2025-06-24T14:39:45.123456+00:00\",\n    \"parser_version\": 1.0,\n    \"box_id\": \"19\",\n    \"timestamp\": \"2025-06-06T17:34:00+00:00\",\n    \"readings\": [\n        {\"T\": 2483, \"RH\": 5480, \"Noise\": 6389},\n        {\"T\": 2482, \"RH\": 5483, \"Noise\": 5181},\n        ...\n    ],\n    \"event\": \"&lt;event-name&gt;\",\n    \"published_at\": \"2025-06-24T14:39:44.391Z\",\n    \"coreid\": \"&lt;core-id&gt;\"\n}\n</code></pre></p> <p>Each blob represents one webhook delivery containing 15 minutes of a single sensor readings.</p> <p>For parsing logic and validation checks see Function Apps Reference. For blob organization see Blob Storage Reference.</p> <p> Blob Storage \u21e2  Database Writer \u2192  SQL Database</p> <p>Database Writer function triggers automatically every 30 minutes. It scans the <code>environment/incoming/</code> folder and retrieves all unprocessed blobs. After processing, blobs are moved to <code>archived/</code> folder, ensuring only new data remains in <code>incoming/</code>.</p> <p>The function processes all sensor data collected during that period in a single batch. For each blob, it calculates individual timestamps for each reading (adding 1 minute increments from the batch start time), converts raw values to final units (temperature to Fahrenheit, humidity to percentage, noise to dB), and computes heat index using the NWS equation.</p> <p>Data format: Batch of records prepared for database insertion: <pre><code>[\n    {\n    \"box_id\": \"19\",\n    \"deployment_id\": 42,\n    \"timestamp\": \"2025-06-06T17:34:00+00:00\",\n    \"temperature\": 76.69,  # Converted to Fahrenheit\n    \"humidity\": 54.80,     # Converted to percentage\n    \"heat_index\": 77.2,    # Calculated from T and RH\n    \"noise\": 59.90,        # Converted to dB\n    \"quality_ok\": true,\n    \"source_blob\": \"2025-06-06_17-34_box-19\"\n    },\n    ...\n]\n</code></pre></p> <p>Records are written to the database using batch INSERT statements: <pre><code>INSERT INTO nu_readings (\n    deployment_id, box_id, timestamp,\n    temperature, humidity, heat_index, noise,\n    quality_ok, source_blob\n)\nVALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n</code></pre></p> <p>For processing logic and deduplication see Function Apps Reference. For schema details see Complete Schema Reference.</p> <p> SQL Database \u21e2  API \u2192  Public Facing Dashboard</p> <p>Dashboard sends HTTP requests to the API with query parameters specifying sensor (deployment_id), metric (temperature, humidity, heat_index, or noise), time range (start_date, end_date), and aggregation level (1min, 1hour, or 1day).</p> <p>The API validates parameters and queries the database. Based on the aggregation level, it either retrieves raw 1-minute readings or calculates hourly/daily averages using SQL aggregation functions.</p> <p>Query example for hourly aggregated data: <pre><code>SELECT\n    DATEADD(HOUR, DATEDIFF(HOUR, 0, timestamp), 0) AS timestamp,\n    AVG(temperature) AS temperature\nFROM nu_readings\nWHERE deployment_id = 19\n    AND timestamp &gt;= '2025-06-06T17:00:00'\n    AND timestamp &lt;= '2025-06-06T18:00:00'\n    AND temperature IS NOT NULL\nGROUP BY DATEADD(HOUR, DATEDIFF(HOUR, 0, timestamp), 0)\nORDER BY timestamp\n</code></pre></p> <p>The API converts timestamps from UTC to US/Eastern timezone and formats the response as JSON: <pre><code>{\n    \"deployment_id\": 19,\n    \"metric\": \"temperature\",\n    \"readings\": [\n        {\"timestamp\": \"2025-06-06T13:34:00-04:00\", \"temperature\": 76.69},\n        {\"timestamp\": \"2025-06-06T13:35:00-04:00\", \"temperature\": 76.71},\n        ...\n    ]\n}\n</code></pre></p> <p>The dashboard receives the data and renders interactive time-series visualizations using Plotly.js.</p> <p>For API endpoint details see API Endpoints Reference. For dashboard code see Static Web App Reference.</p> <p> SQL Database \u21e2  Daily Reporter</p> <p>Daily Reporter function triggers once per day on a schedule. It queries the database to collect operational statistics from the previous 24-hour period, including total records written, sensor performance metrics, and data gaps.</p> <p>The function analyzes sensor health by calculating the percentage of expected data received for each sensor and identifying those with no data or underperforming (&lt; 85% of expected readings). It also tracks the largest data gap for each sensor and lists uninstalled sensors.</p> <p>Data format: Email report with operational summary: <pre><code>SENSOR DATA DAILY REPORT\nReport Date: 2026-02-03 (Eastern Time)\nReport Period: 2026-02-03T05:00:00+00:00 to 2026-02-04T05:00:00+00:00 (UTC)\n\n=== SUMMARY ===\nInstalled Sensors: 53/55\nTotal Records Written: 41,565\nSensors with no data: 16\nSensors Underperforming (&lt; 85.0%): 14\nSensors Performing OK (\u2265 85.0%): 23\n\n=== SENSORS WITH NO DATA (Installed) ===\n  02, 08, 09, ...\n\n=== SENSORS UNDERPERFORMING (&lt; 85.0% EXPECTED DATA) ===\n  \u2022 Sensor 04: 30.2% | Largest gap: 4.5 hours\n  \u2022 Sensor 05: 46.9% | Largest gap: 12.0 hours\n  \u2022 Sensor 06: 55.2% | Largest gap: 10.5 hours\n  ...\n\n=== UNINSTALLED SENSORS ===\n  14, 42\n</code></pre></p> <p>The email is sent to the research team for monitoring system health and identifying sensor issues requiring attention.</p> <p>For function code and email configuration see Function Apps Reference.</p>"},{"location":"02-working-with-data/blob-storage/accessing-blobs/","title":"How to access raw JSON","text":""},{"location":"02-working-with-data/blob-storage/blob-structure/","title":"Folder organization, file naming","text":""},{"location":"02-working-with-data/database/common-queries/","title":"Copy-paste SQL examples","text":""},{"location":"02-working-with-data/database/connecting/","title":"Connection strings, auth, network access","text":""},{"location":"02-working-with-data/database/data-quality/","title":"Flags, duplicates, validation","text":""},{"location":"02-working-with-data/database/understanding-schema/","title":"Understanding the Schema","text":"<p>This page explains the database schema, including table relationships, column purposes, and key concepts for working with sensor data.</p> <p>For complete technical specifications including table creation statements and DDL code, see Complete Schema Reference.</p>"},{"location":"02-working-with-data/database/understanding-schema/#overview","title":"Overview","text":"<p>The database contains five main tables:</p> <ul> <li>nu_sensors - Metadata about sensor deployments and locations</li> <li>nu_readings - Environmental sensor readings (temperature, humidity, noise)</li> <li>nu_errors - Error messages from sensors</li> <li>nu_startup - Sensor boot/restart logs</li> <li>nu_quality_issues - Data quality problems and manual corrections</li> </ul>"},{"location":"02-working-with-data/database/understanding-schema/#nu_sensors-metadata","title":"nu_sensors (Metadata)","text":"<p>This table tracks sensor deployments. A \"deployment\" represents a specific sensor (box_id) installed at a specific location during a specific time period. When a sensor is moved to a new location, it gets a new deployment_id.</p>"},{"location":"02-working-with-data/database/understanding-schema/#columns","title":"Columns","text":"<p>deployment_id (PK, int, not null) Unique identifier for this deployment. Referenced by nu_readings to link sensor data to physical locations.</p> <p>box_id (int, not null) Physical sensor identifier (1-55). This number is printed on the sensor hardware and appears in sensor messages.</p> <p>location_id (int, null) Optional reference to a standardized location. Currently not used.</p> <p>coreid (nvarchar(255), not null) Particle device ID - unique hardware identifier from Particle.io platform. Used to validate webhook data comes from legitimate sensors.</p> <p>location_address (nvarchar(255), not null) Human-readable address or landmark description of where the sensor is installed (e.g., \"Siever St &amp; Tiffany Moore Tot Park\").</p> <p>latitude (decimal(10,8), not null) Geographic latitude of sensor location in decimal degrees.</p> <p>longitude (decimal(11,8), not null) Geographic longitude of sensor location in decimal degrees.</p> <p>install_datetime (datetime2(7), not null) UTC timestamp when the sensor was installed at this location.</p> <p>is_active (bit, null) Current deployment status: 1 = active, 0 = deactivated. Only one deployment per box_id should have is_active = 1 at any time.</p> <p>created_at (datetime2(7), null) UTC timestamp when this database record was created (may differ from install_datetime).</p> <p>uninstall_datetime (datetime2(7), null) UTC timestamp when the sensor was removed from this location. NULL for currently active deployments.</p>"},{"location":"02-working-with-data/database/understanding-schema/#example-record","title":"Example Record","text":"<pre><code>deployment_id:      1\nbox_id:             1\nlocation_id:        NULL\ncoreid:             e00fce688d4f56458911d5b9\nlocation_address:   Siever st &amp; Tiffany Moore Tot Park\nlatitude:           42.30900000\nlongitude:          -71.09100000\ninstall_datetime:   2025-06-20 13:17:00\nis_active:          1\ncreated_at:         2025-10-28 20:42:19\nuninstall_datetime: NULL\n</code></pre> <p>This shows sensor Box 1 actively deployed at Siever St since June 20, 2025.</p>"},{"location":"02-working-with-data/database/understanding-schema/#indexes","title":"Indexes","text":"<p>UQ_active_box_id (Unique filtered index on box_id where is_active = 1) Ensures only one active deployment exists per sensor at any time. This index prevents accidentally marking multiple deployments as active for the same box_id, which would cause the Database Writer to fail when trying to determine which deployment_id to use for incoming data.</p>"},{"location":"02-working-with-data/database/understanding-schema/#key-concepts","title":"Key Concepts","text":"<p>Deployment vs Sensor: A sensor (box_id) can have multiple deployments over time. When sensor Box 1 moves from Location A to Location B, it gets two deployment records - the first with uninstall_datetime filled, the second with is_active = 1.</p> <p>Active Deployments: The Database Writer function queries this table to find the current deployment_id for incoming sensor data based on box_id and is_active = 1.</p> <p>Location History: Historical deployments (is_active = 0, uninstall_datetime populated) preserve location history for data analysis.</p> <p>Managing Deployments: When adding new deployments or marking sensors as uninstalled, existing readings may need their deployment_id or quality_ok flags updated retroactively. See Database Changes for procedures on adding deployments, uninstalling sensors, and correcting historical data.</p>"},{"location":"02-working-with-data/database/understanding-schema/#nu_readings-environmental-data","title":"nu_readings (Environmental Data)","text":"<p>This table stores environmental sensor readings with one record per minute per sensor. This is the primary data table for research analysis and powers the public dashboard visualizations.</p>"},{"location":"02-working-with-data/database/understanding-schema/#columns_1","title":"Columns","text":"<p>id (PK, bigint, not null) Unique identifier for each reading. Auto-incrementing to handle large data volumes (billions of records over time).</p> <p>deployment_id (FK, int, not null) Links reading to sensor deployment in nu_sensors table. Determines which physical location the reading came from.</p> <p>box_id (int, not null) Physical sensor identifier (1-55). Duplicated from deployment for query performance - allows fast filtering without joining to nu_sensors.</p> <p>timestamp (datetime2(7), not null) UTC timestamp when the reading was collected by the sensor. Each sensor produces one reading per minute.</p> <p>temperature (float, null) Temperature in Fahrenheit. NULL if sensor malfunction or data quality issue detected.</p> <p>humidity (float, null) Relative humidity as percentage (0-100). NULL if sensor malfunction or data quality issue detected.</p> <p>noise (float, null) Noise level in decibels (dB). NULL if sensor malfunction or data quality issue detected.</p> <p>heat_index (float, null) Calculated heat index in Fahrenheit using temperature and humidity. Computed by Database Writer using the NWS equation.</p> <p>source_blob (nvarchar(255), not null) Filename of the blob containing the raw sensor data for this reading. Enables tracing back to original data if issues arise.</p> <p>created_at (datetime2(7), null) UTC timestamp when this record was written to the database. Used by Daily Reporter to track data pipeline performance.</p> <p>quality_ok (bit, null) Data quality flag: 1 = valid data, 0 = questionable data. Set to 0 when sensor issues detected or manual corrections needed. See nu_quality_issues table for details.</p>"},{"location":"02-working-with-data/database/understanding-schema/#example-record_1","title":"Example Record","text":"<pre><code>id:             1\ndeployment_id:  10\nbox_id:         10\ntimestamp:      2025-05-03 23:17:00\ntemperature:    50.5\nhumidity:       92.1\nnoise:          40.4\nheat_index:     50.04\nsource_blob:    10_20251028T144801Z\ncreated_at:     2025-11-05 15:58:40\nquality_ok:     0\n</code></pre> <p>This shows a reading from Box 10 collected at 23:17 UTC on May 3rd. Note quality_ok = 0, indicating this reading has been flagged for data quality issues (see nu_quality_issues table for details).</p>"},{"location":"02-working-with-data/database/understanding-schema/#indexes_1","title":"Indexes","text":"<p>IX_readings_deployment_id_timestamp Optimizes API queries that filter by deployment and time range - the most common query pattern for dashboard requests.</p> <p>IX_readings_box_id Enables fast lookups by sensor identifier without needing to join to nu_sensors.</p> <p>IX_readings_created_box Supports Daily Reporter queries that analyze recently written data grouped by sensor.</p> <p>UK_readings_box_timestamp (Unique constraint) Prevents duplicate readings for the same sensor at the same timestamp. Critical for data integrity during Database Writer processing.</p>"},{"location":"02-working-with-data/database/understanding-schema/#key-concepts_1","title":"Key Concepts","text":"<p>One Reading Per Minute: Sensors collect data every minute, so a healthy sensor produces 1,440 readings per day (24 hours \u00d7 60 minutes).</p> <p>Denormalized box_id: While deployment_id provides the foreign key relationship, box_id is duplicated here for query performance. This avoids joining to nu_sensors for simple box-based queries.</p> <p>Quality Flagging: The quality_ok flag marks problematic data without deleting it. Researchers can choose whether to include quality_ok=0 records in their analysis.</p> <p>Data Volume: With 55 active sensors, this table grows by approximately ~79,000 records per day (~2.4M per month).</p> <p>Relationship to Deployments: When a sensor moves locations, new readings use the new deployment_id. Historical readings retain their original deployment_id, preserving accurate location history.</p>"},{"location":"02-working-with-data/database/understanding-schema/#nu_quality_issues-data-quality-tracking","title":"nu_quality_issues (Data Quality Tracking)","text":"<p>This table tracks known data quality problems for specific sensors during specific time periods. When issues are recorded here, the Database Writer automatically flags affected readings in nu_readings with quality_ok = 0.</p>"},{"location":"02-working-with-data/database/understanding-schema/#columns_2","title":"Columns","text":"<p>id (PK, bigint, not null) Unique identifier for each quality issue record.</p> <p>box_id (int, not null) Sensor experiencing the quality issue.</p> <p>start_datetime (datetime2(7), not null) UTC timestamp when the quality issue began.</p> <p>end_datetime (datetime2(7), null) UTC timestamp when the issue was resolved. NULL indicates an ongoing issue.</p> <p>issue_type (nvarchar(50), not null) Category of quality issue. Common types: 'clock_drift' (timestamp problems), 'sensor_malfunction' (hardware issues), 'connectivity' (communication problems).</p> <p>issue_description (nvarchar(500), null) Optional detailed notes about the issue, useful for understanding context during data analysis.</p> <p>is_resolved (computed, int, not null) Automatically calculated: 1 if end_datetime is set (issue resolved), 0 if NULL (ongoing issue).</p>"},{"location":"02-working-with-data/database/understanding-schema/#example-record_2","title":"Example Record","text":"<pre><code>id:                  1\nbox_id:              1\nstart_datetime:      2025-06-20 13:17:00\nend_datetime:        NULL\nissue_type:          clock_drift\nissue_description:   Timestamp off by less than 1 hour\nis_resolved:         0\n</code></pre> <p>This shows Box 1 has an ongoing clock drift issue since June 20, 2025.</p>"},{"location":"02-working-with-data/database/understanding-schema/#key-concepts_2","title":"Key Concepts","text":"<p>Automatic Quality Flagging: When the Database Writer processes sensor data, it queries this table for unresolved issues (is_resolved = 0). Any readings from affected sensors get quality_ok = 0 in nu_readings.</p> <p>Resolving Issues: When a sensor issue is fixed, set end_datetime to mark when the problem ended. The is_resolved column automatically updates to 1.</p> <p>Historical Tracking: Resolved issues (is_resolved = 1) remain in the table to document data quality history. This helps researchers understand why certain time periods have quality_ok = 0.</p> <p>Impact on Analysis: Researchers can choose to exclude quality_ok = 0 readings from analysis, or investigate them separately. The issue_type and issue_description fields provide context for making these decisions.</p> <p>Manual Flagging: Quality issues can be added retroactively. When added, a separate script can update existing nu_readings records to set quality_ok = 0 for the affected time period. See Database Changes for procedures.</p>"},{"location":"02-working-with-data/database/understanding-schema/#additional-tables","title":"Additional Tables","text":"<p>The database also contains nu_errors and nu_startup tables for diagnostic and troubleshooting purposes. These tables are rarely queried during normal data analysis workflows and are primarily used for:</p> <ul> <li>Tracing when sensors began malfunctioning</li> <li>Investigating connectivity issues</li> <li>Debugging sensor firmware problems</li> <li>Tracking sensor restart patterns</li> </ul> <p>For complete specifications of these tables, see Complete Schema Reference.</p>"},{"location":"04-making-changes/database-changes/","title":"Database Changes","text":"<p>This page covers procedures for modifying existing database records, including sensor deployments, quality issues, and retroactive data corrections.</p> <p>Prerequisites: This page assumes you have:</p> <ul> <li>Successfully connected to the database (see Connecting to Database)</li> <li>Write permissions on the database (contact IT if uncertain)</li> <li>Configured network access if needed (see SQL Database Reference)</li> <li>A database management tool or programmatic access (see Connection Tools)</li> </ul> <p>All queries shown can be executed either through a database manager (Azure Data Studio, SSMS) or programmatically using Python, PowerShell, or other database clients.</p>"},{"location":"04-making-changes/database-changes/#sensor-deployment-management","title":"Sensor Deployment Management","text":""},{"location":"04-making-changes/database-changes/#adding-new-sensor-deployments","title":"Adding New Sensor Deployments","text":"<p>Sensor Tracking Spreadsheet</p> <p>Physical sensor relocations require tracking in both the database and the master sensor tracking spreadsheet maintained by the research team. Before creating a new deployment:</p> <ol> <li>Contact the research team for access to the sensor tracking spreadsheet</li> <li>Update the spreadsheet with the new deployment information (usually done by the research team)</li> <li>Create the new deployment record in the database following the procedure above</li> </ol> <p>This ensures consistency between operational tracking and database records.</p> <p>When a sensor is moved to a new location, you need to close the previous deployment and create a new one. This is a two-step process that must be done in order.</p> <p>Prerequisites:</p> <ul> <li>Know the box_id of the sensor being moved</li> <li>Have the new location details (address, coordinates, coreid)</li> <li>Have the exact install_datetime for the new location</li> <li>Have the uninstall_datetime for the previous location</li> </ul> <p>Constraints:</p> <ul> <li>Only one deployment per box_id can have <code>is_active = 1</code> at a time (enforced by unique index)</li> <li>The <code>deployment_id</code> is auto-incrementing and cannot be manually assigned</li> <li>You cannot add a new active deployment if the previous one is still marked <code>is_active = 1</code></li> </ul> <p>Procedure:</p> <p>Step 1: Close the previous deployment</p> <p>Set the old deployment to inactive and record when it was uninstalled:</p> <pre><code>```sql\nUPDATE nu_sensors\nSET \n    is_active = 0,\n    uninstall_datetime = '2025-11-07 00:00:00'  -- UTC timestamp when sensor was removed\nWHERE box_id = 9 AND is_active = 1;\n```\n</code></pre> <p>Step 2: Add the new deployment</p> <p>Insert the new deployment record with the new location details:</p> <pre><code>```sql\nINSERT INTO nu_sensors (box_id, location_id, coreid, location_address, latitude, longitude, install_datetime)\nVALUES (\n    9,                                    -- Same box_id\n    NULL,                                 -- location_id (optional, verify with the research team its value)\n    'e00fce68cd4bb76b1b85dbe7',           -- Particle device coreid\n    'Savin &amp; Tupelo',                     -- New location address\n    42.31670,                             -- New latitude\n    -71.08095,                            -- New longitude\n    '2025-10-31 11:59:00'                 -- UTC timestamp when sensor installed at new location\n);\n```\n</code></pre> <p>Notes:</p> <ul> <li>The new record automatically gets the next available <code>deployment_id</code> (you cannot specify it)</li> <li>The new record defaults to <code>is_active = 1</code> and <code>created_at = GETUTCDATE()</code></li> <li>If Step 1 fails or is skipped, Step 2 will fail with a unique constraint violation</li> </ul> <p>Verification:</p> <p>Check that the deployment was added correctly:</p> <pre><code>```sql\nSELECT deployment_id, box_id, location_address, install_datetime, is_active\nFROM nu_sensors\nWHERE box_id = 9\nORDER BY deployment_id DESC;\n```\n</code></pre> <p>You should see:</p> <ul> <li>Old deployment with <code>is_active = 0</code> and <code>uninstall_datetime</code> populated</li> <li>New deployment with <code>is_active = 1</code> and <code>uninstall_datetime = NULL</code></li> </ul> <p>After Adding Deployment:</p> <p>If data was already recorded under the old deployment_id after the sensor was moved, you'll need to update those readings. See Updating deployment_id for Readings below.</p>"},{"location":"04-making-changes/database-changes/#correcting-deployment-information","title":"Correcting Deployment Information","text":"<p>Use these queries to fix typos or incorrect information in existing deployment records. These updates are for correcting data entry errors only - not for recording sensor movements.</p> <p>Important: If a sensor has physically moved to a new location, you must create a new deployment record (see Adding New Sensor Deployments above). Do not update the existing deployment's location fields.</p> <p>Correcting Location Id:</p> <pre><code>```sql\nUPDATE nu_sensors\nSET location_id = 'Corrected Location Id'\nWHERE deployment_id = 10;\n```\n</code></pre> <p>Correcting Location Address:</p> <pre><code>```sql\nUPDATE nu_sensors\nSET location_address = 'Corrected Address'\nWHERE deployment_id = 10;\n```\n</code></pre> <p>Correcting Coordinates:</p> <pre><code>```sql\nUPDATE nu_sensors\nSET \n    latitude = 42.31234,\n    longitude = -71.09876\nWHERE deployment_id = 10;\n```\n</code></pre> <p>Correcting Install Datetime:</p> <pre><code>```sql\nUPDATE nu_sensors\nSET install_datetime = '2025-10-31 12:00:00'\nWHERE deployment_id = 10;\n```\n</code></pre> <p>Correcting Particle Core ID:</p> <pre><code>```sql\nUPDATE nu_sensors\nSET coreid = 'e00fce68cd4bb76b1b85dbe7'\nWHERE deployment_id = 10;\n```\n</code></pre>"},{"location":"04-making-changes/database-changes/#quality-issue-management","title":"Quality Issue Management","text":""},{"location":"04-making-changes/database-changes/#recording-new-quality-issues","title":"Recording New Quality Issues","text":"<p>When you identify a data quality problem with a sensor, record it in the nu_quality_issues table. This automatically flags future readings from that sensor with <code>quality_ok = 0</code> during Database Writer processing.</p> <p>Common Issue Types:</p> <ul> <li><code>clock_drift</code> - Sensor timestamp significantly off from actual time</li> <li><code>sensor_malfunction</code> - Hardware reporting implausible or erratic values</li> <li><code>connectivity</code> - Intermittent communication problems causing data gaps</li> <li><code>deployment_error</code> - Incorrect metadata or location information</li> </ul> <p>Adding a Quality Issue:</p> <pre><code>```sql\nINSERT INTO nu_quality_issues (box_id, start_datetime, issue_type, issue_description)\nVALUES (\n    19,                                    -- Sensor with the issue\n    '2025-11-15 08:00:00',                 -- UTC timestamp when issue began\n    'sensor_malfunction',                  -- Issue category\n    'Temperature readings stuck at 32\u00b0F'   -- Detailed description\n);\n```\n</code></pre> <p>Notes:</p> <ul> <li>Leave <code>end_datetime</code> as NULL - it will be set when the issue is resolved</li> <li>The <code>is_resolved</code> column automatically computes to 0 (unresolved)</li> <li>Future data from this sensor will be flagged with <code>quality_ok = 0</code></li> <li>Existing data is NOT automatically flagged - see Flagging Data During Quality Issue Periods for retroactive flagging</li> </ul> <p>Writing Clear Descriptions:</p> <p>Good issue descriptions help researchers understand data quality problems:</p> <pre><code>```sql\n-- Good: Specific and actionable\n'Temperature readings consistently 10\u00b0F higher than nearby sensors'\n\n-- Good: Clear timeframe context\n'No data received after firmware update at 14:30 UTC'\n\n-- Poor: Too vague\n'Something wrong with sensor'\n```\n</code></pre>"},{"location":"04-making-changes/database-changes/#resolving-quality-issues","title":"Resolving Quality Issues","text":"<p>When a sensor issue is fixed (hardware replaced, firmware updated, etc.), mark the issue as resolved by setting the <code>end_datetime</code>.</p> <p>Marking an Issue as Resolved:</p> <pre><code>```sql\nUPDATE nu_quality_issues\nSET end_datetime = '2025-11-18 12:00:00'  -- UTC timestamp when issue was fixed\nWHERE id = 5;\n```\n</code></pre> <p>Verification:</p> <p>The <code>is_resolved</code> computed column automatically updates to 1:</p> <pre><code>```sql\nSELECT id, box_id, issue_type, start_datetime, end_datetime, is_resolved\nFROM nu_quality_issues\nWHERE id = 5;\n```\n</code></pre> <p>You should see <code>is_resolved = 1</code> and <code>end_datetime</code> populated.</p> <p>After Resolving:</p> <ul> <li>Future readings from this sensor will no longer be automatically flagged</li> <li>Readings between <code>start_datetime</code> and <code>end_datetime</code> remain flagged with <code>quality_ok = 0</code></li> <li>If readings after <code>end_datetime</code> were already written with <code>quality_ok = 0</code>, you may need to update them - see Unflagging Data After Issue Resolution</li> </ul>"},{"location":"04-making-changes/database-changes/#retroactive-data-corrections","title":"Retroactive Data Corrections","text":"<p>These procedures handle scenarios where database records need updating after data has already been written. Common situations include delayed deployment updates, retroactive quality issue recording, or discovering data problems after the fact.</p> <p>Test First</p> <p>Retroactive corrections can affect large numbers of records. Always:</p> <ol> <li>Run a SELECT query first to see how many records will be affected</li> <li>Verify the date ranges and box_id filters are correct</li> <li>Consider backing up affected data before major updates</li> <li>Test on a small subset if possible</li> </ol>"},{"location":"04-making-changes/database-changes/#updating-deployment_id-for-readings","title":"Updating deployment_id for Readings","text":"<p>Scenario: You moved a sensor to a new location but didn't add the new deployment record until days later. Readings were written using the old deployment_id, but they actually came from the new location.</p> <p>Solution: Update readings to use the correct deployment_id based on the install_datetime.</p> <p>Step 1: Identify affected readings</p> <p>Find how many readings need updating:</p> <pre><code>```sql\nSELECT COUNT(*) as affected_readings\nFROM nu_readings r\nJOIN nu_sensors s_old ON r.deployment_id = s_old.deployment_id\nJOIN nu_sensors s_new ON r.box_id = s_new.box_id\nWHERE r.box_id = 9\nAND s_new.deployment_id = 56  -- New deployment\nAND r.timestamp &gt;= s_new.install_datetime  -- Readings after new install\nAND r.deployment_id = s_old.deployment_id  -- Still using old deployment\nAND s_old.deployment_id != s_new.deployment_id;\n```\n</code></pre> <p>Step 2: Update the readings</p> <pre><code>```sql\nUPDATE nu_readings\nSET deployment_id = 56  -- New deployment_id\nWHERE box_id = 9\nAND timestamp &gt;= '2025-10-31 11:59:00'  -- install_datetime of new deployment\nAND deployment_id = 9;  -- Old deployment_id\n```\n</code></pre> <p>Example: Box 9 moved to new location on Oct 31 at 11:59 UTC (new deployment_id = 45), but the deployment record wasn't added until Nov 5. All readings from Oct 31 - Nov 5 were written with the old deployment_id = 44. This query reassigns those ~7,200 readings to the correct deployment.</p>"},{"location":"04-making-changes/database-changes/#flagging-data-after-sensor-uninstall","title":"Flagging Data After Sensor Uninstall","text":"<p>Scenario: A sensor was uninstalled, but continued transmitting data temporarily (cached readings or delayed shutoff). This \"ghost data\" should be flagged as unreliable.</p> <p>Solution: Flag all readings after uninstall_datetime as quality_ok = 0 for the specific deployment.</p> <p>Step 1: Identify the deployment and affected readings</p> <p>First, find the deployment_id for the uninstalled sensor:</p> <pre><code>```sql\nSELECT deployment_id, box_id, location_address, install_datetime, uninstall_datetime\nFROM nu_sensors\nWHERE box_id = 9\nAND uninstall_datetime IS NOT NULL\nORDER BY deployment_id DESC;\n```\n</code></pre> <p>Then check how many readings are affected:</p> <pre><code>```sql\nSELECT COUNT(*) as readings_after_uninstall\nFROM nu_readings r\nJOIN nu_sensors s ON r.deployment_id = s.deployment_id\nWHERE r.deployment_id = 9  -- Specific deployment that was uninstalled\nAND s.uninstall_datetime IS NOT NULL\nAND r.timestamp &gt; s.uninstall_datetime\nAND r.quality_ok = 1;\n```\n</code></pre> <p>Step 2: Flag the readings</p> <pre><code>```sql\nUPDATE nu_readings\nSET quality_ok = 0\nWHERE deployment_id = 9  -- Use specific deployment_id, not box_id\nAND timestamp &gt; '2025-11-07 00:00:00'  -- uninstall_datetime from nu_sensors\nAND quality_ok = 1;\n```\n</code></pre> <p>Why use deployment_id: A box_id can have multiple deployments over time. Using deployment_id ensures you only flag readings from the specific uninstalled deployment, not from earlier or later deployments of the same sensor.</p>"},{"location":"04-making-changes/database-changes/#flagging-data-during-quality-issue-periods","title":"Flagging Data During Quality Issue Periods","text":"<p>Scenario: You discovered a sensor was malfunctioning last week and added a quality issue record today. Readings from last week are already in the database with quality_ok = 1, but should be flagged.</p> <p>Solution: Update readings within the quality issue time range to quality_ok = 0.</p> <p>Step 1: Identify affected readings</p> <pre><code>```sql\nSELECT COUNT(*) as affected_readings\nFROM nu_readings r\nJOIN nu_quality_issues qi ON r.box_id = qi.box_id\nWHERE qi.id = 5  -- Quality issue record ID\nAND r.timestamp &gt;= qi.start_datetime\nAND (qi.end_datetime IS NULL OR r.timestamp &lt;= qi.end_datetime)\nAND r.quality_ok = 1;\n```\n</code></pre> <p>Step 2: Flag the readings</p> <pre><code>```sql\nUPDATE nu_readings\nSET quality_ok = 0\nWHERE box_id = 19\nAND timestamp &gt;= '2025-11-15 08:00:00'  -- start_datetime from quality issue\nAND timestamp &lt;= '2025-11-18 12:00:00'  -- end_datetime from quality issue (or remove this line if ongoing)\nAND quality_ok = 1;\n```\n</code></pre>"},{"location":"04-making-changes/database-changes/#unflagging-data-after-issue-resolution","title":"Unflagging Data After Issue Resolution","text":"<p>Scenario: A quality issue was resolved days ago (end_datetime set), but you just ran the Database Writer which flagged new readings because it checked for unresolved issues before you updated end_datetime. Or readings after the resolution were already in the database when you added the quality issue.</p> <p>Solution: Set quality_ok = 1 for readings after the issue was resolved.</p> <p>Step 1: Identify affected readings</p> <pre><code>```sql\nSELECT COUNT(*) as incorrectly_flagged\nFROM nu_readings r\nJOIN nu_quality_issues qi ON r.box_id = qi.box_id\nWHERE qi.id = 5  -- Quality issue that was resolved\nAND qi.end_datetime IS NOT NULL  -- Issue is resolved\nAND r.timestamp &gt; qi.end_datetime  -- Readings after resolution\nAND r.quality_ok = 0;\n```\n</code></pre> <p>Step 2: Unflag the readings</p> <pre><code>```sql\nUPDATE nu_readings\nSET quality_ok = 1\nWHERE box_id = 19\nAND timestamp &gt; '2025-11-18 12:00:00'  -- end_datetime from quality issue\nAND quality_ok = 0;\n```\n</code></pre> <p>Warning: This assumes no OTHER quality issues affect this sensor. Check for other active issues:</p> <pre><code>```sql\nSELECT id, issue_type, start_datetime, end_datetime, is_resolved\nFROM nu_quality_issues\nWHERE box_id = 19\nAND id != 5  -- Exclude the issue we just resolved\nAND is_resolved = 0;  -- Check for other unresolved issues\n```\n</code></pre> <p>If other unresolved issues exist for this sensor, unflagging all quality_ok = 0 readings may be incorrect.</p>"},{"location":"05-reference/complete-schema/","title":"Complete Schema Reference","text":"<p>This page provides the complete technical specification of the database schema, including all tables, columns, data types, constraints, and indexes. This is the authoritative reference for database structure.</p> <p>For a practical guide to working with the schema, see Understanding the Schema.</p>"},{"location":"05-reference/complete-schema/#table-overview","title":"Table Overview","text":"<p>The database contains five main tables:</p> Table Purpose Primary Key nu_sensors Sensor deployment metadata and locations deployment_id nu_readings Environmental sensor readings id nu_errors Error messages from sensors id nu_startup Sensor boot/restart logs id nu_quality_issues Data quality problems and corrections id <p>All timestamp columns use <code>DATETIME2(7)</code> and store values in UTC.</p>"},{"location":"05-reference/complete-schema/#nu_sensors","title":"nu_sensors","text":"<p>Tracks sensor deployments. Each deployment represents a specific sensor (box_id) installed at a specific location during a specific time period.</p>"},{"location":"05-reference/complete-schema/#table-creation","title":"Table Creation","text":"<pre><code>CREATE TABLE nu_sensors (\n    deployment_id INT IDENTITY(1,1) PRIMARY KEY,\n    box_id INT NOT NULL,\n    location_id INT NULL,\n    coreid NVARCHAR(255) NOT NULL,\n    location_address NVARCHAR(255) NOT NULL,\n    latitude DECIMAL(10, 8) NOT NULL,\n    longitude DECIMAL(11, 8) NOT NULL,\n    install_datetime DATETIME2 NOT NULL,\n    is_active BIT DEFAULT 1,\n    created_at DATETIME2 DEFAULT GETUTCDATE(),\n    uninstall_datetime DATETIME2 NULL\n);\n</code></pre>"},{"location":"05-reference/complete-schema/#indexes","title":"Indexes","text":"<pre><code>-- Ensures only one active deployment per sensor\nCREATE UNIQUE INDEX UQ_active_box_id \nON nu_sensors (box_id) \nWHERE is_active = 1;\n</code></pre>"},{"location":"05-reference/complete-schema/#column-specifications","title":"Column Specifications","text":"Column Type Nullable Description deployment_id INT NOT NULL Primary key, auto-incrementing unique identifier box_id INT NOT NULL Physical sensor identifier (1-55) location_id INT NULL Optional standardized location reference (unused) coreid NVARCHAR(255) NOT NULL Particle device ID for webhook validation location_address NVARCHAR(255) NOT NULL Human-readable location description latitude DECIMAL(10,8) NOT NULL Geographic latitude in decimal degrees longitude DECIMAL(11,8) NOT NULL Geographic longitude in decimal degrees install_datetime DATETIME2 NOT NULL UTC timestamp of sensor installation is_active BIT NULL Deployment status: 1=active, 0=inactive (default: 1) created_at DATETIME2 NULL UTC timestamp when record was created (default: GETUTCDATE()) uninstall_datetime DATETIME2 NULL UTC timestamp when sensor was removed"},{"location":"05-reference/complete-schema/#constraints-and-business-rules","title":"Constraints and Business Rules","text":"<ul> <li>Only one deployment per box_id can have <code>is_active = 1</code> (enforced by UQ_active_box_id index)</li> <li>When a sensor is moved, the old deployment gets <code>is_active = 0</code> and <code>uninstall_datetime</code> populated</li> <li>Database Writer queries this table to map box_id to deployment_id for incoming data</li> </ul>"},{"location":"05-reference/complete-schema/#nu_readings","title":"nu_readings","text":"<p>Stores environmental sensor readings with one record per minute per sensor. This is the primary data table queried by the API and dashboard.</p>"},{"location":"05-reference/complete-schema/#table-creation_1","title":"Table Creation","text":"<pre><code>CREATE TABLE nu_readings (\n    id BIGINT IDENTITY(1,1) PRIMARY KEY,\n    deployment_id INT NOT NULL,\n    box_id INT NOT NULL,\n    timestamp DATETIME2 NOT NULL,\n    temperature FLOAT NULL,\n    humidity FLOAT NULL,\n    noise FLOAT NULL,\n    heat_index FLOAT NULL,\n    source_blob NVARCHAR(255) NOT NULL,\n    created_at DATETIME2 DEFAULT GETUTCDATE(),\n    quality_ok BIT DEFAULT 1,\n    CONSTRAINT UK_readings_box_timestamp \n        UNIQUE (box_id, timestamp),\n    FOREIGN KEY (deployment_id) REFERENCES nu_sensors(deployment_id)\n);\n</code></pre>"},{"location":"05-reference/complete-schema/#indexes_1","title":"Indexes","text":"<pre><code>-- Query performance for API requests (deployment + time range)\nCREATE INDEX IX_readings_deployment_id_timestamp \nON nu_readings (deployment_id, timestamp);\n\n-- Lookup by box_id\nCREATE INDEX IX_readings_box_id \nON nu_readings (box_id);\n\n-- Daily report queries (recent data by sensor)\nCREATE INDEX IX_readings_created_box\nON nu_readings (created_at, box_id);\n</code></pre>"},{"location":"05-reference/complete-schema/#column-specifications_1","title":"Column Specifications","text":"Column Type Nullable Description id BIGINT NOT NULL Primary key, auto-incrementing unique identifier deployment_id INT NOT NULL Foreign key to nu_sensors, links reading to location box_id INT NOT NULL Physical sensor identifier for quick lookups timestamp DATETIME2(7) NOT NULL UTC timestamp when reading was collected temperature FLOAT NULL Temperature in Fahrenheit humidity FLOAT NULL Relative humidity as percentage (0-100) noise FLOAT NULL Noise level in decibels (dB) heat_index FLOAT NULL Calculated heat index in Fahrenheit source_blob NVARCHAR(255) NOT NULL Blob filename containing raw data for traceability created_at DATETIME2 NULL UTC timestamp when record was written to database quality_ok BIT NULL Data quality flag: 1=valid, 0=questionable (default: 1)"},{"location":"05-reference/complete-schema/#constraints-and-business-rules_1","title":"Constraints and Business Rules","text":"<ul> <li>UK_readings_box_timestamp: Prevents duplicate readings for same sensor at same timestamp</li> <li>Foreign Key: deployment_id must exist in nu_sensors table</li> <li>Sensor values can be NULL if sensor malfunction or data corruption detected</li> <li>quality_ok flag set to 0 when data quality issues identified (see nu_quality_issues table)</li> <li>Box_id duplicates deployment_id information for query performance (denormalized)</li> </ul>"},{"location":"05-reference/complete-schema/#typical-record-volume","title":"Typical Record Volume","text":"<p>With 55 sensors collecting data every minute:</p> <ul> <li>~~79,000 readings per day</li> <li>~~2.4M readings per month</li> </ul>"},{"location":"05-reference/complete-schema/#nu_quality_issues","title":"nu_quality_issues","text":"<p>Tracks known data quality problems for specific sensors during specific time periods. Used to automatically flag questionable data in nu_readings table.</p>"},{"location":"05-reference/complete-schema/#table-creation_2","title":"Table Creation","text":"<pre><code>CREATE TABLE nu_quality_issues (\n    id BIGINT IDENTITY(1,1) PRIMARY KEY,\n    box_id INT NOT NULL,\n    start_datetime DATETIME2 NOT NULL,\n    end_datetime DATETIME2 NULL,\n    issue_type NVARCHAR(50) NOT NULL,\n    issue_description NVARCHAR(500) NULL,\n    is_resolved AS CASE WHEN end_datetime IS NOT NULL THEN 1 ELSE 0 END PERSISTED\n);\n</code></pre>"},{"location":"05-reference/complete-schema/#indexes_2","title":"Indexes","text":"<pre><code>-- Query unresolved issues by sensor\nCREATE INDEX IX_quality_issues_box_resolved \nON nu_quality_issues (box_id, is_resolved);\n\n-- Filter by issue type\nCREATE INDEX IX_quality_issues_type \nON nu_quality_issues (issue_type);\n\n-- Find issues overlapping with specific time ranges\nCREATE INDEX IX_quality_issues_datetime \nON nu_quality_issues (start_datetime, end_datetime);\n</code></pre>"},{"location":"05-reference/complete-schema/#column-specifications_2","title":"Column Specifications","text":"Column Type Nullable Description id BIGINT NOT NULL Primary key, auto-incrementing unique identifier box_id INT NOT NULL Sensor with the quality issue start_datetime DATETIME2 NOT NULL UTC timestamp when issue began end_datetime DATETIME2 NULL UTC timestamp when issue resolved (NULL if ongoing) issue_type NVARCHAR(50) NOT NULL Issue category (e.g., 'clock_drift', 'sensor_malfunction', 'connectivity') issue_description NVARCHAR(500) NULL Optional detailed notes about the issue is_resolved INT (computed) NOT NULL Computed: 1 if end_datetime is set, 0 if ongoing"},{"location":"05-reference/complete-schema/#constraints-and-business-rules_2","title":"Constraints and Business Rules","text":"<ul> <li>is_resolved is a computed column - automatically updates based on end_datetime</li> <li>Multiple overlapping issues can exist for the same sensor</li> <li>Database Writer checks this table before inserting readings to set quality_ok flag</li> </ul>"},{"location":"05-reference/complete-schema/#common-issue-types","title":"Common Issue Types","text":"<ul> <li><code>clock_drift</code> - Sensor timestamp off by significant amount</li> <li><code>sensor_malfunction</code> - Hardware reporting implausible values</li> <li><code>connectivity</code> - Intermittent communication problems</li> <li><code>deployment_error</code> - Incorrect metadata or location information</li> </ul>"},{"location":"05-reference/complete-schema/#nu_errors","title":"nu_errors","text":"<p>Stores error messages reported by sensors. Used primarily for diagnostics and troubleshooting sensor malfunctions.</p>"},{"location":"05-reference/complete-schema/#table-creation_3","title":"Table Creation","text":"<pre><code>CREATE TABLE nu_errors (\n    id BIGINT IDENTITY(1,1) PRIMARY KEY,\n    box_id INT NOT NULL,\n    timestamp DATETIME2 NOT NULL,\n    error_code NVARCHAR(50) NULL,\n    source_blob NVARCHAR(255) NOT NULL,\n    created_at DATETIME2 DEFAULT GETUTCDATE(),\n    CONSTRAINT UK_error_readings_box_timestamp \n        UNIQUE (box_id, timestamp)\n);\n</code></pre>"},{"location":"05-reference/complete-schema/#indexes_3","title":"Indexes","text":"<pre><code>-- Query errors by sensor and time range\nCREATE INDEX IX_error_box_timestamp \nON nu_errors (box_id, timestamp);\n</code></pre>"},{"location":"05-reference/complete-schema/#column-specifications_3","title":"Column Specifications","text":"Column Type Nullable Description id BIGINT NOT NULL Primary key, auto-incrementing unique identifier box_id INT NOT NULL Sensor that reported the error timestamp DATETIME2 NOT NULL UTC timestamp when error occurred error_code NVARCHAR(50) NULL Error code or type from sensor firmware source_blob NVARCHAR(255) NOT NULL Blob filename containing original error message created_at DATETIME2 NULL UTC timestamp when record was written to database"},{"location":"05-reference/complete-schema/#constraints-and-business-rules_3","title":"Constraints and Business Rules","text":"<ul> <li>UK_error_readings_box_timestamp: Prevents duplicate error records for same sensor at same timestamp</li> <li>Errors are written by Database Writer when processing error message blobs from Particle webhooks</li> <li>Used primarily for troubleshooting - helps identify when sensors began malfunctioning</li> </ul>"},{"location":"05-reference/complete-schema/#nu_startup","title":"nu_startup","text":"<p>Stores sensor startup/boot events. Used to track when sensors restart, which can indicate power issues, firmware updates, or connectivity problems.</p>"},{"location":"05-reference/complete-schema/#table-creation_4","title":"Table Creation","text":"<pre><code>CREATE TABLE nu_startup (\n    id BIGINT IDENTITY(1,1) PRIMARY KEY,\n    box_id INT NOT NULL,\n    timestamp DATETIME2 NOT NULL,\n    source_blob NVARCHAR(255) NOT NULL,\n    created_at DATETIME2 DEFAULT GETUTCDATE(),\n    CONSTRAINT UK_startup_readings_box_timestamp \n        UNIQUE (box_id, timestamp)\n);\n</code></pre>"},{"location":"05-reference/complete-schema/#indexes_4","title":"Indexes","text":"<pre><code>-- Query startup events by sensor and time range\nCREATE INDEX IX_startup_box_timestamp \nON nu_startup (box_id, timestamp);\n</code></pre>"},{"location":"05-reference/complete-schema/#column-specifications_4","title":"Column Specifications","text":"Column Type Nullable Description id BIGINT NOT NULL Primary key, auto-incrementing unique identifier box_id INT NOT NULL Sensor that restarted timestamp DATETIME2 NOT NULL UTC timestamp when sensor booted source_blob NVARCHAR(255) NOT NULL Blob filename containing original startup message created_at DATETIME2 NULL UTC timestamp when record was written to database"},{"location":"05-reference/complete-schema/#constraints-and-business-rules_4","title":"Constraints and Business Rules","text":"<ul> <li>UK_startup_readings_box_timestamp: Prevents duplicate startup records for same sensor at same timestamp</li> <li>Startup events written by Database Writer when processing startup message blobs from Particle webhooks</li> <li>Frequent restarts may indicate power supply issues, connectivity problems, or environmental conditions affecting sensor hardware</li> <li>Useful for correlating data gaps in nu_readings with sensor restart patterns</li> </ul>"}]}